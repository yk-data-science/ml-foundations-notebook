{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51e0168f",
   "metadata": {},
   "source": [
    "# Numeric Scaling\n",
    "\n",
    "---\n",
    "\n",
    "## What is Numeric Scaling?\n",
    "\n",
    "Numeric scaling is a preprocessing technique that transforms numerical features to a common scale without distorting differences in the ranges of values.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Why Scaling Matters\n",
    "\n",
    "Some machine learning algorithms are sensitive to the scale of numerical features. Without proper scaling, these algorithms may perform poorly or converge slowly.\n",
    "\n",
    "### Algorithms affected by feature scaling include:\n",
    "\n",
    "- **Support Vector Machines (SVM):**  \n",
    "  SVM uses distance-based calculations to find the optimal hyperplane. Features with larger scales can dominate the distance metrics, leading to biased results.\n",
    "\n",
    "- **k-Nearest Neighbors (k-NN):**  \n",
    "  k-NN relies on distance metrics (e.g., Euclidean distance) to find neighbors. Features on larger scales will disproportionately influence neighbor selection.\n",
    "\n",
    "- **Neural Networks:**  \n",
    "  Large feature scales can cause gradients to explode or vanish, making training unstable or slow.\n",
    "\n",
    "Proper scaling ensures all features contribute equally to the learning process, improving model performance and training stability.\n",
    "\n",
    "---\n",
    "\n",
    "## Example Data\n",
    "\n",
    "Consider the following sample dataset of two features:\n",
    "\n",
    "| Sample | Feature 1 | Feature 2 |\n",
    "|--------|-----------|-----------|\n",
    "| 1      | 10        | 1000      |\n",
    "| 2      | 15        | 1500      |\n",
    "| 3      | 20        | 2000      |\n",
    "| 4      | 25        | 2500      |\n",
    "| 5      | 30        | 3000      |\n",
    "\n",
    "Because Feature 1 and Feature 2 are on very different scales, many machine learning algorithms will struggle without scaling.\n",
    "\n",
    "---\n",
    "\n",
    "## Common Scaling Methods\n",
    "\n",
    "### 1. Min-Max Scaling (Normalization)\n",
    "\n",
    "Scales features to a fixed range, usually [0, 1].\n",
    "\n",
    "**Formula:**  \n",
    "X' = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "Sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0360216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.  ]\n",
      " [0.25 0.25]\n",
      " [0.5  0.5 ]\n",
      " [0.75 0.75]\n",
      " [1.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[10, 1000],\n",
    "                 [15, 1500],\n",
    "                 [20, 2000],\n",
    "                 [25, 2500],\n",
    "                 [30, 3000]])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a198a3b",
   "metadata": {},
   "source": [
    "## 2. Standardization (Z-score Normalization)\n",
    "\n",
    "Standardization is a scaling technique that transforms numerical features so that they have a mean of zero and a standard deviation of one (unit variance).\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "X' = (X - μ) / σ\n",
    "\n",
    "- X: Original value  \n",
    "- μ: Mean of the feature  \n",
    "- σ: Standard deviation of the feature  \n",
    "- X': Standardized value\n",
    "\n",
    "---\n",
    "\n",
    "### Characteristics:\n",
    "\n",
    "- Centers the data around zero (mean = 0).  \n",
    "- Scales the data to have unit variance (standard deviation = 1).  \n",
    "- Less sensitive to outliers compared to min-max scaling.  \n",
    "- Useful for algorithms that assume normally distributed data, such as linear regression, logistic regression, and neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "872f3d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.41421356 -1.41421356]\n",
      " [-0.70710678 -0.70710678]\n",
      " [ 0.          0.        ]\n",
      " [ 0.70710678  0.70710678]\n",
      " [ 1.41421356  1.41421356]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[10, 1000],\n",
    "                 [15, 1500],\n",
    "                 [20, 2000],\n",
    "                 [25, 2500],\n",
    "                 [30, 3000]])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded47a5",
   "metadata": {},
   "source": [
    "## 3. Robust Scaling\n",
    "\n",
    "Robust Scaling is a scaling technique that uses statistics which are robust to outliers — the median and the interquartile range (IQR) — instead of the mean and standard deviation.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula:\n",
    "\n",
    "X' = (X - median(X)) / IQR(X)\n",
    "\n",
    "- X: Original value  \n",
    "- median(X): Median of the feature  \n",
    "- IQR(X): Interquartile range (Q3 - Q1) of the feature  \n",
    "- X': Scaled value\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Characteristics:\n",
    "\n",
    "- Centers data around the median instead of the mean.  \n",
    "- Scales data according to the IQR (difference between 75th and 25th percentile).  \n",
    "- More robust to outliers than standardization and min-max scaling.  \n",
    "- Useful when data contains many outliers or is not normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af509121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.  -1. ]\n",
      " [-0.5 -0.5]\n",
      " [ 0.   0. ]\n",
      " [ 0.5  0.5]\n",
      " [98.   1. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[10, 1000],\n",
    "                 [15, 1500],\n",
    "                 [20, 2000],\n",
    "                 [25, 2500],\n",
    "                 [1000, 3000]])  # Outlier in the first column\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
